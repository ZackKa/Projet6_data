# Partie 1

## D√©finition API et test local

### Seattle Energy Prediction - Mod√®le 8 colonnes
üìÅ Arborescence du projet

```bash
projet/
‚îÇ
‚îú‚îÄ‚îÄ save_model.py          # Script pour sauvegarder le mod√®le sklearn dans BentoML
‚îú‚îÄ‚îÄ service.py             # API avec validation Pydantic et endpoint /predict
‚îú‚îÄ‚îÄ bentofile.yaml         # Recette pour cr√©er l‚Äôimage Docker
‚îú‚îÄ‚îÄ energy_model_8cols.joblib  # Mod√®le pipeline sklearn sauvegard√© (8 colonnes)
‚îú‚îÄ‚îÄ requirements.txt       # D√©pendances Python (ou via Poetry)
‚îú‚îÄ‚îÄ 2016_Building_Energy_Benchmarking.csv  # Csv de base
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ template_modelistation_supervisee-Copy1.ipynb #Analyse donn√©e et model retenu
```
üìù √âtapes d√©taill√©es
### 1Ô∏è‚É£ Sauvegarde du mod√®le sklearn

Entra√Ænement et s√©lection des 8 colonnes :

```python
cols_selected = [
    "PropertyGFATotal",
    "NumberofFloors",
    "NumberofBuildings",
    "PropertyGFAParking",
    "BuildingAge",
    "FloorsPer1000GFA",
    "IsLargeBuilding",
    "NumUseTypes"
]

X_reduced = df3_encoded[cols_selected]
y_reduced = df3_encoded['SiteEnergyUse_clipped']

from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import Pipeline

preprocessor_reduced = ColumnTransformer(
    transformers=[('num', StandardScaler(), cols_selected)]
)

rf_model = RandomForestRegressor(n_estimators=200, max_depth=None, random_state=42)

pipeline_reduced = Pipeline([
    ('preprocess', preprocessor_reduced),
    ('model', rf_model)
])

pipeline_reduced.fit(X_reduced, y_reduced)
```

Sauvegarde du mod√®le avec joblib :

```python
import joblib

joblib.dump(pipeline_reduced, "energy_model_8cols.joblib")
print("‚úÖ Mod√®le 8 colonnes sauvegard√© avec succ√®s")
```


###2Ô∏è‚É£ Sauvegarde du mod√®le dans BentoML

Fichier save_model.py :

```python
import joblib
import bentoml

MODEL_PATH = "energy_model_8cols.joblib"
BENTO_MODEL_NAME = "seattle_energy_model_8cols"

API_FEATURES = [
    "PropertyGFATotal",
    "NumberofFloors",
    "IsLargeBuilding",
    "FloorsPer1000GFA",
    "PropertyGFAParking",
    "BuildingAge",
    "NumberofBuildings",
    "NumUseTypes"
]

# Charger le mod√®le sklearn
model = joblib.load(MODEL_PATH)

# Sauvegarder dans BentoML
bentoml.sklearn.save_model(
    name=BENTO_MODEL_NAME,
    model=model,
    metadata={
        "target": "SiteEnergyUse_clipped",
        "unit": "kBtu",
        "api_features": API_FEATURES,
        "description": "RandomForest optimis√© pour pr√©dire la consommation √©nerg√©tique des b√¢timents non r√©sidentiels √† Seattle (8 colonnes)"
    }
)

print("‚úÖ Mod√®le sauvegard√© avec succ√®s dans BentoML")
```

Commande pour ex√©cuter :

```bash
python save_model.py
```


###3Ô∏è‚É£ Cr√©ation de l‚ÄôAPI avec BentoML (service.py)

Chargement du mod√®le BentoML et d√©finition du service :

```python
import bentoml
from bentoml.io import JSON
import pandas as pd
from pydantic import BaseModel, Field, validator

# Charger le mod√®le
model_runner = bentoml.sklearn.get("seattle_energy_model_8cols:latest").to_runner()

# Cr√©er le service BentoML
service = bentoml.Service("seattle_energy_service", runners=[model_runner])
```

D√©finition de la validation Pydantic :

```python
class BuildingData(BaseModel):

    PropertyGFATotal: float = Field(..., gt=0, description="Surface totale du b√¢timent (GFA)")   # BaseModel : classe de base pour cr√©er des mod√®les Pydantic
    NumberofFloors: int = Field(..., gt=0, description="Nombre d'√©tages")                        # Field : d√©finit les champs du mod√®le
    NumberofBuildings: int = Field(..., gt=0, description="Nombre de b√¢timents")                 # le : less than or equal to (inf√©rieur ou √©gal √†)
    PropertyGFAParking: float = Field(..., ge=0, description="Surface de parking")               # ge : greater than or equal to (sup√©rieur ou √©gal √†)
    BuildingAge: int = Field(..., ge=0, le=1000, description="√Çge du b√¢timent")                   # gt : greater than (sup√©rieur √†)
    FloorsPer1000GFA: int = Field(..., gt=0, description="Nombre d'√©tages par 1000 GFA")       # ... : indique que le champ est obligatoire
    IsLargeBuilding: int = Field(..., ge=0, le=1, description="1 si grand b√¢timent, sinon 0")    # description : description du champ
    NumUseTypes: int = Field(..., ge=1, le=100, description="Nombre de types d'usage")
    
    @validator('PropertyGFAParking')
    def parking_must_be_less_than_total(cls, v, values):
        if 'PropertyGFATotal' in values and v > values['PropertyGFATotal']:
            raise ValueError("PropertyGFAParking ne peut pas d√©passer PropertyGFATotal")
        return v
```

Endpoint /predict :

```python
@service.api(input=JSON(pydantic_model=BuildingData), output=JSON())
def predict(building: BuildingData):
    input_df = pd.DataFrame([building.dict()])
    prediction = model_runner.run(input_df)
    return {"prediction_kBtu": float(prediction[0])}
```

Lancer le serveur local pour tester Swagger :

```bash
bentoml serve service.py
```

Swagger disponible : http://localhost:3000

Endpoint /predict accepte un JSON avec les 8 colonnes et renvoie prediction_kBtu.

###4Ô∏è‚É£ Cr√©ation de l‚Äôimage Docker

Fichier bentofile.yaml :

```yaml
service: "service.py:service"

labels:
  owner: "z"
  project: "seattle-energy-prediction"

include:
  - "*.py"
  - "energy_model_8cols.joblib"

python:
  pip_requirements: requirements.txt
```

Commandes :

Build l‚Äôimage Docker :

```bash
bentoml build
```

Cela cr√©e une image Docker dans le store BentoML et g√©n√®re un <TAG_GENERATED>.

Tester avec BentoML (avant Docker) :

```bash
bentoml serve seattle_energy_service:<TAG_GENERATED>
```
Dans mon cas bentoml serve seattle_energy_service:wkg7lfw2qo34mbd4

###5Ô∏è‚É£ Ex√©cution dans Docker

Lancer le container Docker :

```bash
docker run --rm -p 3000:3000 <IMAGE_TAG>
```
Dans mon cas docker run --rm -p 3000:3000 seattle_energy_service:s3xbaqg2q66kmbd4

Tester l‚ÄôAPI via Swagger ou Postman : http://localhost:3000/predict.

###6Ô∏è‚É£ Notes

Mod√®le 8 colonnes : toutes les √©tapes n‚Äôaffectent pas le mod√®le original √† 92 colonnes.

Validation Pydantic : assure que les inputs envoy√©s √† l‚ÄôAPI sont corrects.

Docker : permet de d√©ployer facilement le mod√®le en production.

BentoML store : stocke les versions des mod√®les (latest ou un tag sp√©cifique).


# Partie 2

## üöÄ D√©ploiement d'un mod√®le Machine Learning sur Google Cloud Platform (GCP)

Ce guide d√©crit les √©tapes pour d√©ployer un mod√®le Machine Learning sous forme de service REST sur Google Cloud Run, en utilisant Google Cloud Artifact Registry pour stocker l‚Äôimage Docker.

###1Ô∏è‚É£ Installation et configuration du SDK GCP
(Permet d‚Äôinstaller les outils n√©cessaires pour interagir avec GCP depuis ton terminal)

Installer le Google Cloud SDK.

Lancer la configuration :

```bash
gcloud init
```

Exemple de sortie et interactions :

```bash
Welcome to the Google Cloud CLI! ...
You must sign in to continue. Would you like to sign in (Y/n)? Y
```

Choisir le projet √† utiliser :
```bash
Pick cloud project to use: [1] project-8fcce7ef-47da-4b16-b5b [2] seattle-energy-api-481506 ...
```

R√©pondre 2 pour s√©lectionner seattle-energy-api-481506.

Configurer la r√©gion pour Compute Engine :

```bash
gcloud config set compute/region europe-west1
```

Activer les API si n√©cessaire :

```bash
API [compute.googleapis.com] not enabled on project ... Would you like to enable and retry? (y/N) y
```

V√©rifier la configuration :

```bash
gcloud config list
```

###2Ô∏è‚É£ Activer les services n√©cessaires
(Cloud Run permet de d√©ployer le service REST, Artifact Registry permet de stocker l‚Äôimage Docker que Cloud Run utilisera pour ex√©cuter le mod√®le)

Activer Cloud Run et Artifact Registry :

```bash
gcloud services enable run.googleapis.com
gcloud services enable artifactregistry.googleapis.com
```

V√©rifier que les services sont activ√©s :

```bash
gcloud services list --enabled | findstr run
gcloud services list --enabled | findstr artifact
```

###3Ô∏è‚É£ Cr√©er un d√©p√¥t Artifact Registry
(C‚Äôest ici que l‚Äôon stocke l‚Äôimage Docker. Cloud Run ira la r√©cup√©rer depuis ce d√©p√¥t pour ex√©cuter le service)

Cr√©er le d√©p√¥t Docker :

```bash
gcloud artifacts repositories create seattle-energy-repo \
    --repository-format=docker \
    --location=europe-west1 \
    --description="Repository pour Docker images de Seattle Energy API"
```

V√©rifier le d√©p√¥t :

```bash
gcloud artifacts repositories list --location=europe-west1
```

###4Ô∏è‚É£ Authentification Docker
(Permet √† Docker de se connecter √† Artifact Registry pour pousser l‚Äôimage)

Configurer Docker pour pousser les images sur Artifact Registry :

```bash
gcloud auth configure-docker europe-west1-docker.pkg.dev
```

###5Ô∏è‚É£ Construire et pousser l‚Äôimage Docker
(On cr√©e l‚Äôimage Docker contenant le mod√®le et le code pour le servir, puis on la pousse dans Artifact Registry pour que Cloud Run puisse l‚Äôutiliser)

Tagger l‚Äôimage Docker locale :

```bash
docker tag seattle_energy_service:s3xbaqg2q66kmbd4 \
    europe-west1-docker.pkg.dev/seattle-energy-api-481506/seattle-energy-repo/seattle-energy-service:latest
```

Pousser l‚Äôimage sur Artifact Registry :

```bash
docker push europe-west1-docker.pkg.dev/seattle-energy-api-481506/seattle-energy-repo/seattle-energy-service:latest
```

V√©rifier que l‚Äôimage est bien dans le d√©p√¥t :

```bash
gcloud artifacts docker images list \
    europe-west1-docker.pkg.dev/seattle-energy-api-481506/seattle-energy-repo
```

###6Ô∏è‚É£ D√©ployer l‚Äôimage sur Cloud Run
(Cloud Run va ex√©cuter le service REST en utilisant l‚Äôimage Docker stock√©e dans Artifact Registry. L‚Äôacc√®s public permet √† n‚Äôimporte quel client d‚Äôenvoyer des requ√™tes HTTP au mod√®le)

D√©ployer le service avec acc√®s public :

```bash
gcloud run deploy seattle-energy-service \
    --image europe-west1-docker.pkg.dev/seattle-energy-api-481506/seattle-energy-repo/seattle-energy-service:latest \
    --platform managed \
    --allow-unauthenticated \
    --region europe-west1
```

###7Ô∏è‚É£ Tester le service REST
(On v√©rifie que le service fonctionne correctement en envoyant des donn√©es et en recevant la pr√©diction du mod√®le)

Exemple de requ√™te POST pour tester le mod√®le :

```bash
curl -X POST https://seattle-energy-service-526618594404.europe-west1.run.app/predict \
-H "Content-Type: application/json" \
-d '{
  "PropertyGFATotal": 1000,
  "NumberofFloors": 5,
  "NumberofBuildings": 1,
  "PropertyGFAParking": 100,
  "BuildingAge": 50,
  "FloorsPer1000GFA": 5,
  "IsLargeBuilding": 1,
  "NumUseTypes": 3 
}'
```

‚úÖ Si tout fonctionne, le service retourne la pr√©diction du mod√®le.